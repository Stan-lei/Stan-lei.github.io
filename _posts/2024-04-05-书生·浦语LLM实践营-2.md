---
layout:     post
title:      轻松玩转书生·浦语大模型趣味Demo
subtitle:   书生·浦语LLM实践营-笔记2
date:       2024-04-05
author:     Stanlei
catalog: true
tags:
    - LLM
    - 书生·浦语
---
## 0 课程背景

> - [轻松玩转书生·浦语大模型趣味 Demo](https://www.bilibili.com/video/BV1AH4y1H78d/)
> - [InternLM/Tutorial](https://github.com/InternLM/Tutorial)
> - 主讲人：书生·浦语大模型角色扮演SIG小组长，那路

![image-20240405224743916](/img/posts/2024-04-05-书生·浦语LLM实践营-2.assets/image-20240405224743916.png)

## 1 课程笔记

本节课主要介绍了InternLM2-Chat-1.8B、八戒-Chat-1.8B、Lagent智能体Demo、灵笔InternLM-XComposer2的功能和部署方法。

**InternLM2-Chat-1.8B**

书生·浦语-1.8B (InternLM2-1.8B) 是第二代浦语模型系列的18亿参数版本。为了方便用户使用和研究，书生·浦语-1.8B (InternLM2-1.8B) 共有三个版本的开源模型，他们分别是：

- InternLM2-1.8B: 具有高质量和高适应灵活性的基础模型，为下游深度适应提供了良好的起点。
- InternLM2-Chat-1.8B-SFT：在 InternLM2-1.8B 上进行监督微调 (SFT) 后得到的对话模型。
- InternLM2-Chat-1.8B：通过在线 RLHF 在 InternLM2-Chat-1.8B-SFT 之上进一步对齐。 InternLM2-Chat-1.8B表现出更好的指令跟随、聊天体验和函数调用，推荐下游应用程序使用。

InternLM2 模型具备以下的技术特点

- 有效支持20万字超长上下文：模型在20万字长输入中几乎完美地实现长文“大海捞针”，而且在 LongBench 和 L-Eval 等长文任务中的表现也达到开源模型中的领先水平。
- 综合性能全面提升：各能力维度相比上一代模型全面进步，在推理、数学、代码等方面的能力提升显著。

**八戒-Chat-1.8B**

- **八戒-Chat**是利用《西游记》剧本中所有关于猪八戒的台词和语句，以及Chat-GPT-3.5生成的相关问题结果，基于**InternLM2-chat-1.8b**进行**全量微调**得到的模仿猪八戒语气的聊天语言模型。

**Lagent智能体-demo**

- Lagent 是一个轻量级、开源的基于大语言模型的智能体（agent）框架，支持用户快速地将一个大语言模型转变为多种类型的智能体，并提供了一些典型工具为大语言模型赋能。

![alt text](/img/posts/2024-04-05-书生·浦语LLM实践营-2.assets/Lagent-1.png)

> *来源：[Tutorial/helloworld/hello_world.md at camp2 · InternLM/Tutorial](https://github.com/InternLM/Tutorial/blob/camp2/helloworld/hello_world.md)*
>
> Lagent 的特性总结如下：
>
> - 流式输出：提供 stream_chat 接口作流式输出，本地就能演示酷炫的流式 Demo。
> - 接口统一，设计全面升级，提升拓展性，包括：
>   - Model : 不论是 OpenAI API, Transformers 还是推理加速框架 LMDeploy 一网打尽，模型切换可以游刃有余；
>   - Action: 简单的继承和装饰，即可打造自己个人的工具集，不论 InternLM 还是 GPT 均可适配；
>   - Agent：与 Model 的输入接口保持一致，模型到智能体的蜕变只需一步，便捷各种 agent 的探索实现；
> - 文档全面升级，API 文档全覆盖。

**灵笔InternLM-XComposer2**

**浦语·灵笔2**是基于[书生·浦语2](https://github.com/InternLM/InternLM/tree/main)大语言模型研发的突破性的图文多模态大模型，具有非凡的图文写作和图像理解能力，在多种应用场景表现出色：

- **自由指令输入的图文写作：** 浦语·灵笔2可以理解**自由形式的图文指令输入，包括大纲、文章细节要求、参考图片等**，为用户打造图文并貌的专属文章。生成的文章文采斐然，图文相得益彰，提供沉浸式的阅读体验。
- **准确的图文问题解答：** 浦语·灵笔2具有海量图文知识，可以准确的回复各种图文问答难题，在识别、感知、细节描述、视觉推理等能力上表现惊人。
- **杰出性能：** 浦语·灵笔2基于书生·浦语2-7B模型，在13项多模态评测中大幅领先同量级多模态模型，在其中6项评测中超过 GPT-4V 和 Gemini Pro。

![img](/img/posts/2024-04-05-书生·浦语LLM实践营-2.assets/Benchmark_radar_CN.png)

开源的浦语·灵笔2包括两个版本:

- **InternLM-XComposer2-VL-7B** [🤗](https://huggingface.co/internlm/internlm-xcomposer2-vl-7b) [modelscope](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-vl-7b)（浦语·灵笔2-视觉问答-7B）: 基于书生·浦语2-7B大语言模型训练，面向多模态评测和视觉问答。浦语·灵笔2-视觉问答-7B是目前最强的基于7B量级语言模型基座的图文多模态大模型，领跑多达13个多模态大模型榜单。
- **InternLM-XComposer2-7B** [🤗](https://huggingface.co/internlm/internlm-xcomposer2-7b) [modelscope](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-7b/): 进一步微调，支持自由指令输入图文写作的图文多模态大模型。

## 2 基础作业

![image-20240405230726108](/img/posts/2024-04-05-书生·浦语LLM实践营-2.assets/image-20240405230726108.png)

> 补充：使用streamlit运行八戒-chat

<img src="/img/posts/2024-04-05-书生·浦语LLM实践营-2.assets/image-20240405233720043.png" alt="image-20240405233720043" style="zoom:67%;" />

<img src="/img/posts/2024-04-05-书生·浦语LLM实践营-2.assets/image-20240405233740745.png" alt="image-20240405233740745" style="zoom: 67%;" />

## 3 进阶作业

> - [x] 熟悉 `huggingface` 下载功能，使用 `huggingface_hub` python 包，下载 `InternLM2-Chat-7B` 的 `config.json` 文件到本地（需截图下载过程）
>   - ![image-20240405234735021](/img/posts/2024-04-05-书生·浦语LLM实践营-2.assets/image-20240405234735021.png)
>   - ![image-20240405234409904](/img/posts/2024-04-05-书生·浦语LLM实践营-2.assets/image-20240405234409904.png)
>   - ![image-20240405234551143](/img/posts/2024-04-05-书生·浦语LLM实践营-2.assets/image-20240405234551143.png)
> - [ ] 完成 `浦语·灵笔2` 的 `图文创作` 及 `视觉问答` 部署（需截图）
>   - 本次由于时间原因暂未完成，后续会补做的
> - [ ] 完成 `Lagent` 工具调用 `数据分析` Demo 部署（需截图）
>   - 本次由于时间原因暂未完成，后续会补做的
