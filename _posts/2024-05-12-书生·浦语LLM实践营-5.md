---
layout:     post
title:      LMDeploy量化部署
subtitle:   书生·浦语LLM实践营-笔记5
date:       2024-05-12
author:     Stanlei
catalog: true
tags:
    - LLM
    - Finetune
    - 书生·浦语
---
## 0 课程背景

> - [LMDeploy 量化部署 LLM-VLM 实践](https://www.bilibili.com/video/BV1tr421x75B/)
> - [InternLM/Tutorial](https://github.com/InternLM/Tutorial)
> - 主讲人：书生·浦源挑战赛冠军队伍队长，安泓郡

本节课主要介绍大模型部署的方法和LMDeploy框架的使用。

## 1 课程笔记

**大模型部署的挑战**

- ![image-20240512130321061](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512130321061.png)
- ![image-20240512130343438](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512130343438.png)
- ![image-20240512130406007](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512130406007.png)

**大模型部署方法**

- 模型剪枝(Pruning)
  - ![image-20240512141723597](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512141723597.png)
- 知识蒸馏(Knowledge Distillation，KD)
  - ![image-20240512141735597](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512141735597.png)
- 量化(Quantization)
  - ![image-20240512141753629](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512141753629.png)

**LMDeploy**

- LMDeploy 由 [MMDeploy](https://github.com/open-mmlab/mmdeploy) 和 [MMRazor](https://github.com/open-mmlab/mmrazor) 团队联合开发，是涵盖了 LLM 任务的全套轻量化、部署和服务解决方案。 
  - **高效的推理**：LMDeploy 开发了 Persistent Batch(即 Continuous Batch)，Blocked K/V Cache，动态拆分和融合，张量并行，高效的计算 kernel等重要特性。推理性能是 vLLM 的 1.8 倍
  - **可靠的量化**：LMDeploy 支持权重量化和 k/v 量化。4bit 模型推理效率是 FP16 下的 2.4 倍。量化模型的可靠性已通过 OpenCompass 评测得到充分验证。
  - **便捷的服务**：通过请求分发服务，LMDeploy 支持多模型在多机、多卡上的推理服务。
  - **有状态推理**：通过缓存多轮对话过程中 attention 的 k/v，记住对话历史，从而避免重复处理历史会话。显著提升长文本多轮对话场景中的效率。
- 性能
  - 在各种规模的模型上，每秒处理的请求数是 vLLM 的 1.36 ~ 1.85 倍。在静态推理能力方面，TurboMind 4bit 模型推理速度（out token/s）远高于 FP16/BF16 推理。在小 batch 时，提高到 2.4 倍。
  - ![293825277-8e455cf1-a792-4fa8-91a2-75df96a2a5ba](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/293825277-8e455cf1-a792-4fa8-91a2-75df96a2a5ba.png)
- 支持的模型
  - ![image-20240512142433899](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512142433899.png)

## 2 基础作业

环境配置

- ```shell
  conda activate lmdeploy
  pip install lmdeploy[all]==0.3.0
  ```

- ![image-20240512130831470](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512130831470.png)

- 使用transformer库进行推理

  - ![image-20240512131459835](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512131459835.png)

- 以命令行方式与InternLM2-Chat-1.8B 模型对话（**推理速度明显快得多**）

  - ![image-20240512131644370](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512131644370.png)
  - ![image-20240512131736472](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512131736472.png)
  - ![image-20240512131756012](/img/posts/2024-05-12-书生·浦语LLM实践营-5.assets/image-20240512131756012.png)
